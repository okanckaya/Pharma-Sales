# -*- coding: utf-8 -*-
"""VitrA_X_05042022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tgt0R0SQR0jT3sbA8r7IGqau2ptxkdTO
"""

import numpy as np
import pandas as pd

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeRegressor
import lightgbm as ltb
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.metrics import explained_variance_score

import requests
import json
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from matplotlib.pyplot import figure

from google.colab import drive 
drive.mount('/content/gdrive')

# Importing main dataset:
df = pd.read_csv('gdrive/My Drive/VitrA/Product_X_ALL_LAST.csv', sep=";")

# Importing forecast information obtained from Turkish State Meteorological Service:
df_heat = pd.read_excel('gdrive/My Drive/VitrA/20210914C0A8-Aylık Ortalama Sıcaklık (°C).xlsx')

ProductX = df.copy()
ProductX['Quantity'] = ProductX['Quantity'].fillna(0)

df_heat.head()

df_heat.describe().T

df_heat = df_heat[df_heat.Month != 13]

# The data obtained from Turkish State Meteorological Servicecontains 13th month. So these rows eliminated from dataset.

df_heat.reset_index()

# To be able to merge the main and heatdatasets, indexes of heat datas got reseted.

df_last = pd.merge(df, df_heat)
df_last.reset_index(inplace=False)
df_last

# The main dataset and heat data merged together.

df_last.isna().sum()

ProductX = df_last.copy()
ProductX = ProductX[df.Quantity != 0]
ProductX = ProductX.reset_index(drop=True)

# Rows with 0 Quantity got eliminated from dataset to prevent the error from Mean Absolute Percentage Error.

ProductX.describe().T

ProductX['CPI(Year)']=ProductX['CPI(Year)'].str.replace(',','.')
ProductX['CPI(Month)']=ProductX['CPI(Month)'].str.replace(',','.')
ProductX['StockMarket']=ProductX['StockMarket'].str.replace(',','.')
ProductX['DolarB']=ProductX['DolarB'].str.replace(',','.')
ProductX['DolarS']=ProductX['DolarS'].str.replace(',','.')

# The commas replaced with point to assign the types of some datas as float/int.

ProductX['CPI(Year)']=ProductX['CPI(Year)'].astype(float)
ProductX['CPI(Month)']=ProductX['CPI(Month)'].astype(float)
ProductX['Period'] = ProductX['Period'].astype(int)
ProductX['Year'] = ProductX['Year'].astype(int)
ProductX['Month'] = ProductX['Month'].astype(int)
ProductX['StockMarket'] = ProductX['StockMarket'].astype(float)
ProductX['DolarB'] = ProductX['DolarB'].astype(float)
ProductX['DolarS'] = ProductX['DolarS'].astype(float)
ProductX['Male'] = ProductX['Male'].astype(float)
ProductX['Male_Province'] = ProductX['Male_Province'].astype(float)
ProductX['Male_Rural'] = ProductX['Male_Rural'].astype(float)

ProductX.dtypes

ProductX = ProductX[["Period", "Province", "Month", "Year", "Season", "BigCity", "Region", "StockMarket", "DolarB", "DolarS",
                     "CPI(Year)", "CPI(Month)", "Male", "Male_Province", "Male_Rural", "Trends_Eczacıbaşı", "Trends_Prostat", "Med_Deg", "Quantity"]]

ProductX_p = ProductX[["Province", "Month", "Year", "Season", "BigCity", "Region", "StockMarket", "DolarB", "DolarS",
                     "CPI(Year)", "CPI(Month)", "Male", "Male_Province", "Male_Rural", "Trends_Eczacıbaşı", "Trends_Prostat", "Med_Deg", "Quantity"]]

# Çağlar Hoca's notebook. Description of dataset
def MissingUniqueStatistics(df):
  
  import io
  import pandas as pd
  import psutil, os, gc, time
  import seaborn as sns
  from IPython.display import display, HTML
  from io import BytesIO
  import base64
  
  variable_name_list = []
  total_entry_list = []
  data_type_list = []
  unique_values_list = []
  number_of_unique_values_list = []
  missing_value_number_list = []
  missing_value_ratio_list = []
  mean_list=[]
  std_list=[]
  min_list=[]
  Q1_list=[]
  Q2_list=[]
  Q3_list=[]
  max_list=[]

  df_statistics = df.describe().copy()

  for col in df.columns:

    variable_name_list.append(col)
    total_entry_list.append(df.loc[:,col].shape[0])
    data_type_list.append(df.loc[:,col].dtype)
    unique_values_list.append(list(df.loc[:,col].unique()))
    number_of_unique_values_list.append(len(list(df.loc[:,col].unique())))
    missing_value_number_list.append(df.loc[:,col].isna().sum())
    missing_value_ratio_list.append(round((df.loc[:,col].isna().sum()/df.loc[:,col].shape[0]),4))

    try:
      mean_list.append(df_statistics.loc[:,col][1])
      std_list.append(df_statistics.loc[:,col][2])
      min_list.append(df_statistics.loc[:,col][3])
      Q1_list.append(df_statistics.loc[:,col][4])
      Q2_list.append(df_statistics.loc[:,col][5])
      Q3_list.append(df_statistics.loc[:,col][6])
      max_list.append(df_statistics.loc[:,col][7])
    except:
      mean_list.append('NaN')
      std_list.append('NaN')
      min_list.append('NaN')
      Q1_list.append('NaN')
      Q2_list.append('NaN')
      Q3_list.append('NaN')
      max_list.append('NaN')

  data_info_df = pd.DataFrame({'Variable': variable_name_list, 
                               '#_Total_Entry':total_entry_list,
                               '#_Missing_Value': missing_value_number_list,
                               '%_Missing_Value':missing_value_ratio_list,
                               'Data_Type': data_type_list, 
                               'Unique_Values': unique_values_list,
                               '#_Unique_Values':number_of_unique_values_list,
                               'Mean':mean_list,
                               'STD':std_list,
                               'Min':min_list,
                               'Q1':Q1_list,
                               'Q2':Q2_list,
                               'Q3':Q3_list,
                               'Max':max_list
                               })

  data_info_df = data_info_df.set_index("Variable", inplace=False)

  return data_info_df.sort_values(by='%_Missing_Value', ascending=False)

data_info = MissingUniqueStatistics(ProductX)
data_info

# Heatmap Correlation:

ProductX_corr = ProductX.copy()

# Dealing with categorical imputs:
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
ProductX_corr['Province'] = labelencoder.fit_transform(ProductX_corr['Province'].values)
ProductX_corr['Season'] = labelencoder.fit_transform(ProductX_corr['Season'].values)
ProductX_corr['Region'] = labelencoder.fit_transform(ProductX_corr['Region'].values)

def show_correlations(df, show_chart = True):
    fig = plt.figure(figsize = (25,15))
    corr = df.corr()
    if show_chart == True:
        sns.heatmap(corr, 
                    xticklabels=corr.columns.values,
                    yticklabels=corr.columns.values,
                    annot=True)
    return corr

show_correlations(ProductX_corr)

# It is aimed to see the correlation between different parameters with each other and the Quantity value.

ProductX['Period'].apply(str)
ProductX['Period'] = pd.to_datetime(ProductX['Period'], format = "%Y%m")

sns.set(rc={'figure.figsize':(20,10)})
sns.scatterplot(data=ProductX, x='Period', y='Quantity', legend=False, sizes=(10,200))

# Theoutliers can affect the results significantly. In this section it is aimed to whether eliminate or not the outliers. In later stages of analyzes it can be seen that holding outliers gives more reasonable results.

"""Dealing with Variables:

*   CPI(Year) and Borsa features show high correlation with each other. Beacuse of that the feature (Borsa) which correlates weaker with "Quantity" got eliminated.
*   DolarB and DolarS features show high correlation with each other. Beacuse of that one of them (DolarB) got eliminated.
*   Since the predictions are desired to be made without Province variable it's also got eliminated.
* Since te Season variable also shows weak correlation with Quantity, it's also got eliminated.
*   Since Male, Male_Province and Male_Rural almost indicates the same thing, Male and Male_Rural variables eliminated due to lower correlation with Quantity.
"""

# Giving Back products:

df_givingback= ProductX.loc[ProductX['Quantity']<0]
df_givingback

df_givingback = df_givingback.groupby(['Year','Month','Province']).sum()
df_givingback

# In this section it is aimed to see the count of giving back products. Since they are not too many, it is decided to keep them.

"""***VISUALIZATION*** """

a = ProductX.groupby('Region')["Quantity"].sum()
regionDf = a.to_frame().reset_index()

# Bar plot of Quantity-Region for three years period.

plt.figure(figsize=(12, 10), facecolor="white")
splot=sns.barplot(x="Region",y="Quantity",data=regionDf,)
for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
plt.xlabel("Region", size=14)
plt.ylabel("Quantity", size=14)

yearBased = ProductX.groupby(["Year","Month"])["Quantity"].sum()
yearBased = yearBased.to_frame().reset_index()

year2017 = yearBased.loc[yearBased['Year'] == 2017]
year2018 = yearBased.loc[yearBased['Year'] == 2018]
year2019 = yearBased.loc[yearBased['Year'] == 2019]

# Bar plot of Quantity-Region for the year 2017.

plt.figure(figsize=(12, 10))
splot=sns.barplot(x="Month",y="Quantity",data=year2017)
for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
plt.xlabel("Month (2017)", size=14)
plt.ylabel("Quantity", size=14)

# Bar plot of Quantity-Region for the year 2018.

plt.figure(figsize=(12, 10))
splot=sns.barplot(x="Month",y="Quantity",data=year2018)
for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
plt.xlabel("Month (2018)", size=14)
plt.ylabel("Quantity", size=14)

# Bar plot of Quantity-Region for the year 2019.

plt.figure(figsize=(12, 10))
splot=sns.barplot(x="Month",y="Quantity",data=year2019)
for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
plt.xlabel("Month (2019)", size=14)
plt.ylabel("Quantity", size=14)

"""It can be seen that from the year 2017 to 2019 Product X sold more every year except the excessive decrease on 08-2019 and 09-2019. As a result of the researches, it was thought that a product group similar to Product X was recalled from the market."""

def yearBasedSales(data):  
  plt.figure(figsize=(8, 6))
  splot=sns.barplot(x="Month",y="Quantity",data=data)
  for p in splot.patches:
      splot.annotate(format(p.get_height(), '.1f'), 
                    (p.get_x() + p.get_width() / 2., p.get_height()), 
                    ha = 'center', va = 'center', 
                    xytext = (0, 9), 
                    textcoords = 'offset points')
  plt.xlabel("Region", size=14)
  plt.ylabel("Quantity", size=14)

labels = ['1', '2', '3', '4', '5','6','7','8','9','10','11','12']
sale2017 = year2017['Quantity'].to_list()
sale2018 = year2018['Quantity'].to_list()
sale2019 = year2019['Quantity'].to_list()

x = np.arange(len(labels))  # the label locations
width = 0.35  # the width of the bars

plt.figure(figsize=(40,20))
fig, ax = plt.subplots()


rects1 = ax.bar(x-0.2,sale2017, width, label='2017')
rects2 = ax.bar(x, sale2018, width, label='2018')
rects3 = ax.bar(x+0.2, sale2019, width, label='2019')
# Add some text for labels, title and custom x-axis tick labels, etc.
# tick labels, etc.
ax.set_ylabel('Quantity')
ax.set_xlabel('Month')
ax.grid(False)
ax.set_facecolor('white')
ax.set_title('Sales by month')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


fig.tight_layout()

plt.show()

"""As can be seen from the graph above, an increasing trend is observed in the sales of product X."""

from matplotlib.pyplot import figure

figure(figsize=(14, 8), dpi=80)
plt.plot(year2017['Month'],year2017['Quantity'], label = "2017")
plt.plot(year2018['Month'], year2018['Quantity'], label = "2018")
plt.plot(year2019['Month'], year2019['Quantity'], label = "2019")
plt.title('Quantities for 3 Years')
plt.xlabel('Month')
plt.ylabel('Quantity')
plt.legend()
plt.show()

"""The sudden decrease mentioned above can be seen quite clearly in this chart."""

provinceBased = ProductX.groupby(["Province"])["Quantity"].sum()
provinceBased = provinceBased.to_frame().reset_index()
provinceBased= provinceBased.sort_values(by=['Quantity'], ascending=False)
provinceBased = provinceBased[:10]

provinceBased

# Bar plot of Quantity-Province for three-year period. It can be seen that densely populated metropolitan areas like Istanbul, Ankara, Gaziantep etc. are on the top of the most-sold list.

plt.figure(figsize=(12, 10))
plt.style.use('default')
splot=sns.barplot(x="Province",y="Quantity",data=provinceBased)
for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')

plt.xlabel("Province", size=14)
plt.ylabel("Quantity", size=14)

provinceBased = ProductX.groupby(["Province"])["Quantity"].sum()
provinceBased = provinceBased.to_frame().reset_index()
provinceBased= provinceBased.sort_values(by=['Quantity'], ascending=False)
provinceBased = provinceBased[:10]

provinceBased = ProductX.groupby(["Province","Year"])["Quantity"].sum()
provinceBased = provinceBased.to_frame().reset_index()

provinceBased2017 = provinceBased.loc[provinceBased['Year'] == 2017]
provinceBased2017= provinceBased2017.sort_values(by=['Quantity'], ascending=False)
provinceBased2017 = provinceBased2017[:10]
provinceBased2017

provinceBased2018 = provinceBased.loc[provinceBased['Year'] == 2018]
provinceBased2018= provinceBased2018.sort_values(by=['Quantity'], ascending=False)
provinceBased2018 = provinceBased2018[:10]
provinceBased2018

provinceBased2019 = provinceBased.loc[provinceBased['Year'] == 2019]
provinceBased2019 = provinceBased2019.sort_values(by=['Quantity'], ascending=False)
provinceBased2019 = provinceBased2019[:10]
provinceBased2019

# Bar plot of Quantity-Province for the year 2017.

plt.figure(figsize=(12, 10))
splot=sns.barplot(x="Province",y="Quantity",data=provinceBased2017)
for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
plt.xlabel("Province", size=14)
plt.ylabel("Quantity", size=14)

# Bar plot of Quantity-Province for the year 2018.

plt.figure(figsize=(12, 10))
splot=sns.barplot(x="Province",y="Quantity",data=provinceBased2018)
for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
plt.xlabel("Province", size=14)
plt.ylabel("Quantity", size=14)

# Bar plot of Quantity-Province for the year 2019.

plt.figure(figsize=(12, 10))
splot=sns.barplot(x="Province",y="Quantity",data=provinceBased2019)
for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
plt.xlabel("Province", size=14)
plt.ylabel("Quantity", size=14)

"""In this section, it is aimed to show the sales of Product X on the map of Turkey."""

df_tr_map = pd.read_csv('gdrive/My Drive/VitrA/Product_X_ALL_LAST.csv', sep=";")

df_tr_map.rename(columns={ df_tr_map.columns[1]: "Province" }, inplace = True)

response = requests.get('https://gist.githubusercontent.com/mebaysan/9be56dd1ca5659c0ff7ea5e2b5cf6479/raw/6d7a77d8a2892bd59f401eb87bd82d7f48642a58/turkey-geojson.json')

geojson = response.json()

geoDict = {}
for i in geojson['features']:
    geoDict[i['properties']['name']] = i['id']

ProductVmap = df_tr_map.groupby(['Province','Year'])['Quantity'].sum()
ProductVmap = ProductVmap.reset_index()
ProductVmap17 = ProductVmap[ProductVmap['Year']==2017].groupby(['Province'])['Quantity'].sum()
ProductVmap17 = ProductVmap17.reset_index()
ProductVmap18 = ProductVmap[ProductVmap['Year']==2018].groupby(['Province'])['Quantity'].sum()
ProductVmap18 = ProductVmap18.reset_index()
ProductVmap19 = ProductVmap[ProductVmap['Year']==2019].groupby(['Province'])['Quantity'].sum()
ProductVmap19 = ProductVmap19.reset_index()

ProductVmap['GeoID'] = ProductVmap['Province'].str.capitalize().apply(lambda x: geoDict[x])

ProductVmap17['GeoID'] = ProductVmap17['Province'].str.capitalize().apply(lambda x: geoDict[x])
ProductVmap18['GeoID'] = ProductVmap18['Province'].str.capitalize().apply(lambda x: geoDict[x])
ProductVmap19['GeoID'] = ProductVmap19['Province'].str.capitalize().apply(lambda x: geoDict[x])

fig = px.choropleth_mapbox(
ProductVmap17,  
geojson=geojson,
locations='GeoID',
color='Quantity',                           
color_continuous_scale="Viridis", 
center={'lat': 38.7200, 'lon': 34.0000},
labels={'Quantity': 'Quantity'},
mapbox_style="carto-positron",
zoom=4.8,
opacity=0.5,
custom_data=[ProductVmap17['Province'],
ProductVmap17['Quantity']]
)
# Some make-up for plot
fig.update_layout(
title='Quantity Ordered From Each Province for Product X - 2017',
title_x=0.5
)
# I created my own hover template for on hover event
hovertemp = '<i style="color:red;">Province:</i> %{customdata[0]}<br>'
hovertemp += '<i>Quantity:</i> %{customdata[1]:,f}<br>'
# I set my own hover template
fig.update_traces(hovertemplate=hovertemp) 
fig.show()

fig = px.choropleth_mapbox(
ProductVmap18,  
geojson=geojson,
locations='GeoID',
color='Quantity',                           
color_continuous_scale="Viridis", 
center={'lat': 38.7200, 'lon': 34.0000},
labels={'Quantity': 'Quantity'},
mapbox_style="carto-positron",
zoom=4.8,
opacity=0.5,
custom_data=[ProductVmap18['Province'],
ProductVmap18['Quantity']]
)
# Some make-up for plot
fig.update_layout(
title='Quantity Ordered From Each Province for Product X - 2018',
title_x=0.5
)
# I created my own hover template for on hover event
hovertemp = '<i style="color:red;">Province:</i> %{customdata[0]}<br>'
hovertemp += '<i>Quantity:</i> %{customdata[1]:,f}<br>'
# I set my own hover template
fig.update_traces(hovertemplate=hovertemp) 
fig.show()

fig = px.choropleth_mapbox(
ProductVmap19,  
geojson=geojson,
locations='GeoID',
color='Quantity',                           
color_continuous_scale="Viridis", 
center={'lat': 38.7200, 'lon': 34.0000},
labels={'Quantity': 'Quantity'},
mapbox_style="carto-positron",
zoom=4.8,
opacity=0.5,
custom_data=[ProductVmap19['Province'],
ProductVmap19['Quantity']]
)
# Some make-up for plot
fig.update_layout(
title='Quantity Ordered From Each Province for Product X - 2019',
title_x=0.5
)
# I created my own hover template for on hover event
hovertemp = '<i style="color:red;">Province:</i> %{customdata[0]}<br>'
hovertemp += '<i>Quantity:</i> %{customdata[1]:,f}<br>'
# I set my own hover template
fig.update_traces(hovertemplate=hovertemp) 
fig.show()

# Dealing with categorical imputs:

# Since machine learning algorithms cannot work with categorical variables, they have been transformed into numerical type with the "label encoding" method.
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

ProductX['Province'] = labelencoder.fit_transform(ProductX['Province'].values)
ProductX['Season'] = labelencoder.fit_transform(ProductX['Season'].values)
ProductX['Region'] = labelencoder.fit_transform(ProductX['Region'].values)

ProductX.dtypes

"""*Creating Subdatasets:*

In this section it is aimed to create different datasets with different input variables. Since the different variables with different correlations can affect the results significantly, the least correlated variables wtih Quantity value elimiated from datasets.
"""

ProductX.columns

##################--- DataFrames for Train ---##################

# Dataset with all variables:
X_1 = ProductX.iloc[:2881, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 ,13, 14, 15, 16, 17]].values
y_1 = ProductX.iloc[:2881, 18].values

# Dataset which excluded the Period, DolarB, Male_Rural variables:
X_2 = ProductX.iloc[:2881, [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 ,13, 15, 16, 17]].values
y_2 = ProductX.iloc[:2881, 18].values

# Dataset which excluded the Period, Province, DolarB, Male_Rural variables:
X_3 = ProductX.iloc[:2881, [2, 3, 4, 5, 6, 7, 9, 10, 11, 12 ,13, 15, 16, 17]].values
y_3 = ProductX.iloc[:2881, 18].values

# Dataset which excluded the Period, Province, DolarB, Borsa, Male_Rural variables:
X_4 = ProductX.iloc[:2881, [2, 3, 4, 5, 6, 9, 10, 11 ,12, 13, 15, 16, 17]].values
y_4 = ProductX.iloc[:2881, 18].values

# Dataset which excluded the Period, Province, DolarB, Borsa, Male, Male_Rural variables:
X_5 = ProductX.iloc[:2881, [2, 3, 4, 5, 6, 9, 10, 11 ,13, 15, 16, 17]].values
y_5 = ProductX.iloc[:2881, 18].values

##################--- DataFrames for Test ---##################

# Dataset with all variables:
X_1_2020 = ProductX.iloc[2881:, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 ,13, 14, 15, 16, 17]].values
y_1_2020 = ProductX.iloc[2881:, 18].values

# Dataset which excluded the Period, DolarB, Male_Rural variables:
X_2_2020 = ProductX.iloc[2881:, [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 ,13, 15, 16, 17]].values
y_2_2020 = ProductX.iloc[2881:, 18].values

# Dataset which excluded the Period, Province, DolarB, Male_Rural variables:
X_3_2020 = ProductX.iloc[2881:, [2, 3, 4, 5, 6, 7, 9, 10, 11, 12 ,13, 15, 16, 17]].values
y_3_2020 = ProductX.iloc[2881:, 18].values

# Dataset which excluded the Period, Province, DolarB, Borsa, Male_Rural variables:
X_4_2020 = ProductX.iloc[:2881, [2, 3, 4, 5, 6, 9, 10, 11 ,12, 13, 15, 16, 17]].values
y_4_2020 = ProductX.iloc[:2881, 18].values

# Dataset which excluded the Period, Province, DolarB, Borsa, Male, Male_Rural variables:
X_5_2020 = ProductX.iloc[2881:, [2, 3, 4, 5, 6, 9, 10, 11 ,13, 15, 16, 17]].values
y_5_2020 = ProductX.iloc[2881:, 18].values

"""Since the distributions of the datas are quite different, the input parameters must be brought to the same scale in order for the results to be more reliable. Therefore, in this section, the training and test data  are scaled to scatter around 0, but 2020 datas:"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_1 = scaler.fit_transform(X_1)
X_2 = scaler.fit_transform(X_2)
X_3 = scaler.fit_transform(X_3)
X_4 = scaler.fit_transform(X_4)
X_5 = scaler.fit_transform(X_5)

X_1_2020 = scaler.fit_transform(X_1_2020)
X_2_2020 = scaler.fit_transform(X_2_2020)
X_3_2020 = scaler.fit_transform(X_3_2020)
X_4_2020 = scaler.fit_transform(X_4_2020)
X_5_2020 = scaler.fit_transform(X_5_2020)

# Splitting dataset:
from sklearn.model_selection import train_test_split

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.3, shuffle=False, random_state = 42)
X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.3, shuffle=False, random_state = 42)
X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3, test_size=0.3, shuffle=False, random_state = 42)
X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X_4, y_4, test_size=0.3, shuffle=False, random_state = 42)
X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(X_5, y_5, test_size=0.3, shuffle=False, random_state = 42)

"""*Machine Learning Algorithms*"""

# The distribution of the actual values and the predictions made by the algorithms is intended to be shown not only with numerical error metrics, but also with visual graphics:

def predVisualization(test_pred,y_test):
  x_ax = range(len(test_pred))
  figure(figsize=(50, 10), dpi=80)
  plt.scatter(x_ax, y_test, lw=6, color="blue", label="original")
  plt.scatter(x_ax, test_pred, lw=4, color="red", label="predicted_rtc")
  plt.legend()
  plt.style.use('default')
  plt.show()

# Differnce between :

def errorVisualization(test_pred,y_test):
  diffs = abs(test_pred-y_test)
  x_ax = range(len(test_pred))
  figure(figsize=(50, 10), dpi=80)
  plt.style.use('default')
  plt.plot(x_ax, test_pred-y_test, lw=6, color="blue", label="difference")
  plt.legend()
  plt.show()

def ml_algorithms(X_train, y_train, X_test, y_test, X_pred):
  
  # Linear Regression:

  linear_reg = LinearRegression()
  model_lr = linear_reg.fit(X_train, y_train)
  train_preds_LinReg = linear_reg.predict(X_train)
  test_preds_LinReg = linear_reg.predict(X_test)

  mae_linReg_train = mean_absolute_error(y_train, train_preds_LinReg)
  mae_linReg_test = mean_absolute_error(y_test, test_preds_LinReg)

  mse_linReg_train = mean_squared_error(y_train, train_preds_LinReg)
  mse_linReg_test = mean_squared_error(y_test, test_preds_LinReg) 

  rmse_linReg_train = mean_squared_error(y_train, train_preds_LinReg, squared=False)
  rmse_linReg_test = mean_squared_error(y_test, test_preds_LinReg, squared=False)

  r2_linReg_train = r2_score(y_train, train_preds_LinReg)
  r2_linReg_test = r2_score(y_test, test_preds_LinReg)

  explained_variance_linreg_train = explained_variance_score(y_train, train_preds_LinReg)
  explained_variance_linreg_test = explained_variance_score(y_test, test_preds_LinReg)

  CoV_train_linreg = rmse_linReg_train/np.mean(y_train)
  CoV_test_linreg = rmse_linReg_test/np.mean(y_test)

  predictions2020_lin_reg = linear_reg.predict(X_pred)
  df = pd.DataFrame(predictions2020_lin_reg, columns=['2020_Predictions'])  
  df_linReg_2020_01 = df.iloc[0:81,:].values
  df_linReg_2020_02 = df.iloc[81:163,:].values
  df_linReg_2020_03 = df.iloc[162:244,:].values
  df_linReg_2020_04 = df.iloc[244:325,:].values
  df_linReg_2020_01 = (pd.DataFrame(df_linReg_2020_01)).sum() 
  df_linReg_2020_02 = (pd.DataFrame(df_linReg_2020_02)).sum() 
  df_linReg_2020_03 = (pd.DataFrame(df_linReg_2020_03)).sum() 
  df_linReg_2020_04 = (pd.DataFrame(df_linReg_2020_04)).sum()

  ######################################################

  # KNN:

  knn_reg = KNeighborsRegressor()
  model_knn = knn_reg.fit(X_train, y_train)
  train_preds_knn_reg  = knn_reg.predict(X_train)
  test_preds_knn_reg = knn_reg.predict(X_test)

  mae_knn_reg_train = mean_absolute_error(y_train, train_preds_knn_reg)
  mae_knn_reg_test = mean_absolute_error(y_test, test_preds_knn_reg)

  mse_knn_reg_train = mean_squared_error(y_train, train_preds_knn_reg)
  mse_knn_reg_test = mean_squared_error(y_test, test_preds_knn_reg) 

  rmse_knn_reg_train = mean_squared_error(y_train, train_preds_knn_reg , squared=False)
  rmse_knn_reg_test = mean_squared_error(y_test, test_preds_knn_reg, squared=False)

  r2_knn_reg_train = r2_score(y_train, train_preds_knn_reg)
  r2_knn_reg_test = r2_score(y_test, test_preds_knn_reg)

  explained_variance_knn_train = explained_variance_score(y_train, train_preds_knn_reg)
  explained_variance_knn_test = explained_variance_score(y_test, test_preds_knn_reg)

  CoV_train_knn = rmse_knn_reg_train/np.mean(y_train)
  CoV_test_knn = rmse_knn_reg_test/np.mean(y_test)

  predictions2020_knn_reg = knn_reg.predict(X_pred)
  df_knn = pd.DataFrame(predictions2020_knn_reg, columns=['2020_Predictions'])  
  df_knn_2020_01 = df_knn.iloc[0:81,:].values
  df_knn_2020_02 = df_knn.iloc[81:163,:].values
  df_knn_2020_03 = df_knn.iloc[162:244,:].values
  df_knn_2020_04 = df_knn.iloc[244:325,:].values
  df_knn_2020_01 = (pd.DataFrame(df_knn_2020_01)).sum() 
  df_knn_2020_02 = (pd.DataFrame(df_knn_2020_02)).sum() 
  df_knn_2020_03 = (pd.DataFrame(df_knn_2020_03)).sum() 
  df_knn_2020_04 = (pd.DataFrame(df_knn_2020_04)).sum()

  ######################################################

  # XGBoost:

  xgb_reg = XGBRegressor(objective='reg:squarederror')
  model_xgb = xgb_reg.fit(X_train, y_train)
  train_preds_xgb_reg  = xgb_reg.predict(X_train)
  test_preds_xgb_reg = xgb_reg.predict(X_test)

  mae_xgb_reg_train = mean_absolute_error(y_train, train_preds_xgb_reg)
  mae_xgb_reg_test = mean_absolute_error(y_test, test_preds_xgb_reg)

  mse_xgb_reg_train = mean_squared_error(y_train, train_preds_xgb_reg)
  mse_xgb_reg_test = mean_squared_error(y_test, test_preds_xgb_reg) 

  rmse_xgb_reg_train = mean_squared_error(y_train, train_preds_xgb_reg , squared=False)
  rmse_xgb_reg_test = mean_squared_error(y_test, test_preds_xgb_reg, squared=False)

  r2_xgb_reg_train = r2_score(y_train, train_preds_xgb_reg)
  r2_xgb_reg_test = r2_score(y_test, test_preds_xgb_reg)

  explained_variance_xgb_train = explained_variance_score(y_train, train_preds_xgb_reg)
  explained_variance_xgb_test = explained_variance_score(y_test, test_preds_xgb_reg)

  CoV_train_xgb = rmse_xgb_reg_train/np.mean(y_train)
  CoV_test_xgb = rmse_xgb_reg_test/np.mean(y_test)

  predictions2020_xgb_reg = xgb_reg.predict(X_pred)
  df_xgb = pd.DataFrame(predictions2020_xgb_reg, columns=['2020_Predictions'])  
  df_xgb_2020_01 = df_xgb.iloc[0:81,:].values
  df_xgb_2020_02 = df_xgb.iloc[81:163,:].values
  df_xgb_2020_03 = df_xgb.iloc[162:244,:].values
  df_xgb_2020_04 = df_xgb.iloc[244:325,:].values
  df_xgb_2020_01 = (pd.DataFrame(df_xgb_2020_01)).sum() 
  df_xgb_2020_02 = (pd.DataFrame(df_xgb_2020_02)).sum() 
  df_xgb_2020_03 = (pd.DataFrame(df_xgb_2020_03)).sum() 
  df_xgb_2020_04 = (pd.DataFrame(df_xgb_2020_04)).sum()

  ######################################################

  # Light GBM:

  lgbm_reg = ltb.LGBMRegressor()
  model_lgbm = lgbm_reg.fit(X_train, y_train)
  train_preds_lgbm_reg  = lgbm_reg.predict(X_train)
  test_preds_lgbm_reg = lgbm_reg.predict(X_test)

  mae_lgbm_reg_train = mean_absolute_error(y_train, train_preds_lgbm_reg)
  mae_lgbm_reg_test = mean_absolute_error(y_test, test_preds_lgbm_reg)

  mse_lgbm_reg_train = mean_squared_error(y_train, train_preds_lgbm_reg)
  mse_lgbm_reg_test = mean_squared_error(y_test, test_preds_lgbm_reg) 

  rmse_lgbm_reg_train = mean_squared_error(y_train, train_preds_lgbm_reg , squared=False)
  rmse_lgbm_reg_test = mean_squared_error(y_test, test_preds_lgbm_reg, squared=False)

  r2_lgbm_reg_train = r2_score(y_train, train_preds_lgbm_reg)
  r2_lgbm_reg_test = r2_score(y_test, test_preds_lgbm_reg)

  explained_variance_lgbm_train = explained_variance_score(y_train, train_preds_lgbm_reg)
  explained_variance_lgbm_test = explained_variance_score(y_test, test_preds_lgbm_reg)

  CoV_train_lgbm = rmse_lgbm_reg_train/np.mean(y_train)
  CoV_test_lgbm = rmse_lgbm_reg_test/np.mean(y_test)

  predictions2020_lgbm_reg = lgbm_reg.predict(X_pred)
  df_lgbm = pd.DataFrame(predictions2020_lgbm_reg, columns=['2020_Predictions'])  
  df_lgbm_2020_01 = df_lgbm.iloc[0:81,:].values
  df_lgbm_2020_02 = df_lgbm.iloc[81:163,:].values
  df_lgbm_2020_03 = df_lgbm.iloc[162:244,:].values
  df_lgbm_2020_04 = df_lgbm.iloc[244:325,:].values
  df_lgbm_2020_01 = (pd.DataFrame(df_lgbm_2020_01)).sum() 
  df_lgbm_2020_02 = (pd.DataFrame(df_lgbm_2020_02)).sum() 
  df_lgbm_2020_03 = (pd.DataFrame(df_lgbm_2020_03)).sum() 
  df_lgbm_2020_04 = (pd.DataFrame(df_lgbm_2020_04)).sum()

  ######################################################

  # Random Forest:

  rfr_reg = RandomForestRegressor()
  model_rfr = rfr_reg.fit(X_train, y_train)
  train_preds_rfr_reg  = rfr_reg.predict(X_train)
  test_preds_rfr_reg = rfr_reg.predict(X_test)

  mae_rfr_reg_train = mean_absolute_error(y_train, train_preds_rfr_reg)
  mae_rfr_reg_test = mean_absolute_error(y_test, test_preds_rfr_reg)

  mse_rfr_reg_train = mean_squared_error(y_train, train_preds_rfr_reg)
  mse_rfr_reg_test = mean_squared_error(y_test, test_preds_rfr_reg) 

  rmse_rfr_reg_train = mean_squared_error(y_train, train_preds_rfr_reg , squared=False)
  rmse_rfr_reg_test = mean_squared_error(y_test, test_preds_rfr_reg, squared=False)

  r2_rfr_reg_train = r2_score(y_train, train_preds_rfr_reg)
  r2_rfr_reg_test = r2_score(y_test, test_preds_rfr_reg)

  explained_variance_rfr_train = explained_variance_score(y_train, train_preds_rfr_reg)
  explained_variance_rfr_test = explained_variance_score(y_test, test_preds_rfr_reg)

  CoV_train_rfr = rmse_rfr_reg_train/np.mean(y_train)
  CoV_test_rfr = rmse_rfr_reg_test/np.mean(y_test)

  predictions2020_rfr_reg = rfr_reg.predict(X_pred)
  df_rfr = pd.DataFrame(predictions2020_rfr_reg, columns=['2020_Predictions'])  
  df_rfr_2020_01 = df_rfr.iloc[0:81,:].values
  df_rfr_2020_02 = df_rfr.iloc[81:163,:].values
  df_rfr_2020_03 = df_rfr.iloc[162:244,:].values
  df_rfr_2020_04 = df_rfr.iloc[244:325,:].values
  df_rfr_2020_01 = (pd.DataFrame(df_rfr_2020_01)).sum() 
  df_rfr_2020_02 = (pd.DataFrame(df_rfr_2020_02)).sum() 
  df_rfr_2020_03 = (pd.DataFrame(df_rfr_2020_03)).sum() 
  df_rfr_2020_04 = (pd.DataFrame(df_rfr_2020_04)).sum()

  ######################################################

  # Support Vector Regressor:

  svr_reg = SVR()
  model_svr = svr_reg.fit(X_train, y_train)
  train_preds_svr_reg  = svr_reg.predict(X_train)
  test_preds_svr_reg = svr_reg.predict(X_test)

  mae_svr_reg_train = mean_absolute_error(y_train, train_preds_svr_reg)
  mae_svr_reg_test = mean_absolute_error(y_test, test_preds_svr_reg)

  mse_svr_reg_train = mean_squared_error(y_train, train_preds_svr_reg)
  mse_svr_reg_test = mean_squared_error(y_test, test_preds_svr_reg) 

  rmse_svr_reg_train = mean_squared_error(y_train, train_preds_svr_reg , squared=False)
  rmse_svr_reg_test = mean_squared_error(y_test, test_preds_svr_reg, squared=False)

  r2_svr_reg_train = r2_score(y_train, train_preds_svr_reg)
  r2_svr_reg_test = r2_score(y_test, test_preds_svr_reg)

  explained_variance_svr_train = explained_variance_score(y_train, train_preds_svr_reg)
  explained_variance_svr_test = explained_variance_score(y_test, test_preds_svr_reg)

  CoV_train_svr = rmse_svr_reg_train/np.mean(y_train)
  CoV_test_svr = rmse_svr_reg_test/np.mean(y_test)

  predictions2020_svr_reg = svr_reg.predict(X_pred)
  df_svr = pd.DataFrame(predictions2020_svr_reg, columns=['2020_Predictions'])  
  df_svr_2020_01 = df_svr.iloc[0:81,:].values
  df_svr_2020_02 = df_svr.iloc[81:163,:].values
  df_svr_2020_03 = df_svr.iloc[162:244,:].values
  df_svr_2020_04 = df_svr.iloc[244:325,:].values
  df_svr_2020_01 = (pd.DataFrame(df_svr_2020_01)).sum() 
  df_svr_2020_02 = (pd.DataFrame(df_svr_2020_02)).sum() 
  df_svr_2020_03 = (pd.DataFrame(df_svr_2020_03)).sum() 
  df_svr_2020_04 = (pd.DataFrame(df_svr_2020_04)).sum()

  ######################################################

  # Gauss NB:

  gauss_reg = GaussianNB()
  model_gauss = gauss_reg.fit(X_train, y_train)
  train_preds_gauss_reg  = gauss_reg.predict(X_train)
  test_preds_gauss_reg = gauss_reg.predict(X_test)

  mae_gauss_reg_train = mean_absolute_error(y_train, train_preds_gauss_reg)
  mae_gauss_reg_test = mean_absolute_error(y_test, test_preds_gauss_reg)

  mse_gauss_reg_train = mean_squared_error(y_train, train_preds_gauss_reg)
  mse_gauss_reg_test = mean_squared_error(y_test, test_preds_gauss_reg) 

  rmse_gauss_reg_train = mean_squared_error(y_train, train_preds_gauss_reg , squared=False)
  rmse_gauss_reg_test = mean_squared_error(y_test, test_preds_gauss_reg, squared=False)

  r2_gauss_reg_train = r2_score(y_train, train_preds_gauss_reg)
  r2_gauss_reg_test = r2_score(y_test, test_preds_gauss_reg)

  explained_variance_gauss_train = explained_variance_score(y_train, train_preds_gauss_reg)
  explained_variance_gauss_test = explained_variance_score(y_test, test_preds_gauss_reg)

  CoV_train_gauss = rmse_gauss_reg_train/np.mean(y_train)
  CoV_test_gauss = rmse_gauss_reg_test/np.mean(y_test)

  predictions2020_gauss_reg = gauss_reg.predict(X_pred)
  df_gauss = pd.DataFrame(predictions2020_gauss_reg, columns=['2020_Predictions'])  
  df_gauss_2020_01 = df_gauss.iloc[0:81,:].values
  df_gauss_2020_02 = df_gauss.iloc[81:163,:].values
  df_gauss_2020_03 = df_gauss.iloc[162:244,:].values
  df_gauss_2020_04 = df_gauss.iloc[244:325,:].values
  df_gauss_2020_01 = (pd.DataFrame(df_gauss_2020_01)).sum() 
  df_gauss_2020_02 = (pd.DataFrame(df_gauss_2020_02)).sum() 
  df_gauss_2020_03 = (pd.DataFrame(df_gauss_2020_03)).sum() 
  df_gauss_2020_04 = (pd.DataFrame(df_gauss_2020_04)).sum()

  ######################################################

  # Decision Trees:

  dtr_reg = DecisionTreeRegressor()
  model_dtr = dtr_reg.fit(X_train, y_train)
  train_preds_dtr_reg  = dtr_reg.predict(X_train)
  test_preds_dtr_reg = dtr_reg.predict(X_test)

  mae_dtr_reg_train = mean_absolute_error(y_train, train_preds_dtr_reg)
  mae_dtr_reg_test = mean_absolute_error(y_test, test_preds_dtr_reg)

  mse_dtr_reg_train = mean_squared_error(y_train, train_preds_dtr_reg)
  mse_dtr_reg_test = mean_squared_error(y_test, test_preds_dtr_reg) 

  rmse_dtr_reg_train = mean_squared_error(y_train, train_preds_dtr_reg , squared=False)
  rmse_dtr_reg_test = mean_squared_error(y_test, test_preds_dtr_reg, squared=False)

  r2_dtr_reg_train = r2_score(y_train, train_preds_dtr_reg)
  r2_dtr_reg_test = r2_score(y_test, test_preds_dtr_reg)

  explained_variance_dtr_train = explained_variance_score(y_train, train_preds_dtr_reg)
  explained_variance_dtr_test = explained_variance_score(y_test, test_preds_dtr_reg)

  CoV_train_dtr = rmse_dtr_reg_train/np.mean(y_train)
  CoV_test_dtr = rmse_dtr_reg_test/np.mean(y_test)

  predictions2020_dtr_reg = dtr_reg.predict(X_pred)
  df_dtr = pd.DataFrame(predictions2020_dtr_reg, columns=['2020_Predictions'])  
  df_dtr_2020_01 = df_dtr.iloc[0:81,:].values
  df_dtr_2020_02 = df_dtr.iloc[81:163,:].values
  df_dtr_2020_03 = df_dtr.iloc[162:244,:].values
  df_dtr_2020_04 = df_dtr.iloc[244:325,:].values
  df_dtr_2020_01 = (pd.DataFrame(df_dtr_2020_01)).sum() 
  df_dtr_2020_02 = (pd.DataFrame(df_dtr_2020_02)).sum() 
  df_dtr_2020_03 = (pd.DataFrame(df_dtr_2020_03)).sum() 
  df_dtr_2020_04 = (pd.DataFrame(df_dtr_2020_04)).sum()

  ######################################################

  # Gradient Boosting:

  gbr_reg = GradientBoostingRegressor()
  model_gbr = gbr_reg.fit(X_train, y_train)
  train_preds_gbr_reg  = gbr_reg.predict(X_train)
  test_preds_gbr_reg = gbr_reg.predict(X_test)

  mae_gbr_reg_train = mean_absolute_error(y_train, train_preds_gbr_reg)
  mae_gbr_reg_test = mean_absolute_error(y_test, test_preds_gbr_reg)

  mse_gbr_reg_train = mean_squared_error(y_train, train_preds_gbr_reg)
  mse_gbr_reg_test = mean_squared_error(y_test, test_preds_gbr_reg) 

  rmse_gbr_reg_train = mean_squared_error(y_train, train_preds_gbr_reg , squared=False)
  rmse_gbr_reg_test = mean_squared_error(y_test, test_preds_gbr_reg, squared=False)

  r2_gbr_reg_train = r2_score(y_train, train_preds_gbr_reg)
  r2_gbr_reg_test = r2_score(y_test, test_preds_gbr_reg)

  explained_variance_gbr_train = explained_variance_score(y_train, train_preds_gbr_reg)
  explained_variance_gbr_test = explained_variance_score(y_test, test_preds_gbr_reg)

  CoV_train_gbr = rmse_gbr_reg_train/np.mean(y_train)
  CoV_test_gbr = rmse_gbr_reg_test/np.mean(y_test)

  predictions2020_gbr_reg = gbr_reg.predict(X_pred)
  df_gbr = pd.DataFrame(predictions2020_gbr_reg, columns=['2020_Predictions'])  
  df_gbr_2020_01 = df_gbr.iloc[0:81,:].values
  df_gbr_2020_02 = df_gbr.iloc[81:163,:].values
  df_gbr_2020_03 = df_gbr.iloc[162:244,:].values
  df_gbr_2020_04 = df_gbr.iloc[244:325,:].values
  df_gbr_2020_01 = (pd.DataFrame(df_gbr_2020_01)).sum()
  df_gbr_2020_02 = (pd.DataFrame(df_gbr_2020_02)).sum() 
  df_gbr_2020_03 = (pd.DataFrame(df_gbr_2020_03)).sum() 
  df_gbr_2020_04 = (pd.DataFrame(df_gbr_2020_04)).sum()

  results = pd.DataFrame({
      'Model': ['Linear Regression', 'KNN', 'XGBoost', 'Light GBM', 'Random Forest', 'Support Vector Machines', 'Gauss NB', 'Decision Trees', 'Gradient Boosting'],
      'Mean Absolute Error (Train)': [mae_linReg_train, mae_knn_reg_train, mae_xgb_reg_train, mae_lgbm_reg_train, mae_rfr_reg_train, mae_svr_reg_train, mae_gauss_reg_train, mae_dtr_reg_train, mae_gbr_reg_train],
      'Mean Absolute Error (Test)': [mae_linReg_test, mae_knn_reg_test, mae_xgb_reg_test, mae_lgbm_reg_test, mae_rfr_reg_test, mae_svr_reg_test, mae_gauss_reg_test, mae_dtr_reg_test, mae_gbr_reg_test],
      'Mean Squared Error (Train)': [mse_linReg_train, mse_knn_reg_train, mse_xgb_reg_train, mse_lgbm_reg_train, mse_rfr_reg_train, mse_svr_reg_train, mse_gauss_reg_train, mse_dtr_reg_train, mse_gbr_reg_train],
      'Mean Squared Error (Test)': [mse_linReg_test, mse_knn_reg_test, mse_xgb_reg_test, mse_lgbm_reg_test, mse_rfr_reg_test, mse_svr_reg_test, mse_gauss_reg_test, mse_dtr_reg_test, mse_gbr_reg_test],
      'Root Mean Squared Error (Train)': [rmse_linReg_train, rmse_knn_reg_train, rmse_xgb_reg_train, rmse_lgbm_reg_train, rmse_rfr_reg_train, rmse_svr_reg_train, rmse_gauss_reg_train, rmse_dtr_reg_train, rmse_gbr_reg_train],
      'Root Mean Squared Error (Test)': [rmse_linReg_test, rmse_knn_reg_test, rmse_xgb_reg_test, rmse_lgbm_reg_test, rmse_rfr_reg_test, rmse_svr_reg_test, rmse_gauss_reg_test, rmse_dtr_reg_test, rmse_gbr_reg_test],
      'R2 (Train)': [r2_linReg_train, r2_knn_reg_train, r2_xgb_reg_train, r2_lgbm_reg_train, r2_rfr_reg_train, r2_svr_reg_train, r2_gauss_reg_train, r2_dtr_reg_train, r2_gbr_reg_train],
      'R2 (Test)': [r2_linReg_test, r2_knn_reg_test, r2_xgb_reg_test, r2_lgbm_reg_test, r2_rfr_reg_test, r2_svr_reg_test, r2_gauss_reg_test, r2_dtr_reg_test, r2_gbr_reg_test],
      'Coefficient of Variance (Train)': [CoV_train_linreg, CoV_train_knn, CoV_train_xgb, CoV_train_lgbm, CoV_train_rfr, CoV_train_svr, CoV_train_gauss, CoV_train_dtr, CoV_train_gbr],
      'Coefficient of Variance (Test)': [CoV_test_linreg, CoV_test_knn, CoV_test_xgb, CoV_test_lgbm, CoV_test_rfr, CoV_test_svr, CoV_test_gauss, CoV_test_dtr, CoV_test_gbr]})
  
  result_df = results.sort_values(by='R2 (Test)', ascending=False)
  result_df = result_df.set_index('Model')
  return result_df

result_df_1 = ml_algorithms(X_train_1, y_train_1, X_test_1, y_test_1, X_1_2020)
result_df_1

result_df_2 = ml_algorithms(X_train_2, y_train_2, X_test_2, y_test_2, X_2_2020)
result_df_2

result_df_3 = ml_algorithms(X_train_3, y_train_3, X_test_3, y_test_3, X_3_2020)
result_df_3

result_df_4 = ml_algorithms(X_train_4, y_train_4, X_test_4, y_test_4, X_4_2020)
result_df_4

result_df_5 = ml_algorithms(X_train_5, y_train_5, X_test_5, y_test_5, X_5_2020)
result_df_5

"""Since the Random Forest, Decision Trees, Gradient Boosting and XGBoost algorithms  give the best results with *Dataset 5* than other algorithm-dataset combinations, grid search was performed to find the best possible result with these algorithm:"""

# Grid Search for Random Forest:

parameters_rfr = {'n_estimators': [100, 200, 300, 500]}

def Random_forest_cv(X_train, y_train, X_test, y_test, X_pred):

  rfr = RandomForestRegressor()
  random_forest_cv = GridSearchCV(estimator=rfr,
                                  param_grid = {'n_estimators': [100, 200, 300, 500],
                                                'criterion': ["squared_error", "absolute_error", "poisson"],
                                                "max_features": ["auto", "sqrt", "log2"]},
                                  cv=3, verbose=0, n_jobs=-1)

  random_forest_cv.fit(X_train, y_train)
  best = random_forest_cv.best_params_

  random_forest_tuned = RandomForestRegressor(parameters = best)
  rfr_tuned =  random_forest_tuned.fit(X_train, y_train)

  train_preds_rfr_reg  = rfr_tuned.predict(X_train)
  test_preds_rfr_reg = rfr_tuned.predict(X_test)

  mae_rfr_reg_train = mean_absolute_error(y_train, train_preds_rfr_reg)
  mae_rfr_reg_test = mean_absolute_error(y_test, test_preds_rfr_reg)

  mse_rfr_reg_train = mean_squared_error(y_train, train_preds_rfr_reg)
  mse_rfr_reg_test = mean_squared_error(y_test, test_preds_rfr_reg) 

  rmse_rfr_reg_train = mean_squared_error(y_train, train_preds_rfr_reg , squared=False)
  rmse_rfr_reg_test = mean_squared_error(y_test, test_preds_rfr_reg, squared=False)

  r2_rfr_reg_train = r2_score(y_train, train_preds_rfr_reg)
  r2_rfr_reg_test = r2_score(y_test, test_preds_rfr_reg)

  explained_variance_rfr_train = explained_variance_score(y_train, train_preds_rfr_reg)
  explained_variance_rfr_test = explained_variance_score(y_test, test_preds_rfr_reg)

  CoV_train_rfr = rmse_rfr_reg_train/np.mean(y_train)
  CoV_test_rfr = rmse_rfr_reg_test/np.mean(y_test)

  predictions2020_rfr_reg = rfr_tuned.predict(X_pred)
  df_rfr = pd.DataFrame(predictions2020_rfr_reg, columns=['2020_Predictions'])  
  df_rfr_2020_01 = df_rfr.iloc[0:81,:].values
  df_rfr_2020_02 = df_rfr.iloc[81:163,:].values
  df_rfr_2020_03 = df_rfr.iloc[162:244,:].values
  df_rfr_2020_04 = df_rfr.iloc[244:325,:].values
  df_rfr_2020_01 = (pd.DataFrame(df_rfr_2020_01)).sum() 
  df_rfr_2020_02 = (pd.DataFrame(df_rfr_2020_02)).sum() 
  df_rfr_2020_03 = (pd.DataFrame(df_rfr_2020_03)).sum() 
  df_rfr_2020_04 = (pd.DataFrame(df_rfr_2020_04)).sum()

  print("Visualization of Predictions for Train:")
  predVisualization(train_preds_rfr_reg, y_train)
  print("Visualization of Predictions for Test:")
  predVisualization(test_preds_rfr_reg, y_test)

  print("Visualization of Error for Train:")
  errorVisualization(train_preds_rfr_reg, y_train)
  print("Visualization of Error for Test:")
  errorVisualization(test_preds_rfr_reg, y_test)

  print("Jan 2020 predictions: ", df_rfr_2020_01)
  print("Feb 2020 predictions: ", df_rfr_2020_02)
  print("March 2020 predictions: ", df_rfr_2020_03)
  print("Apr 2020 predictions: ", df_rfr_2020_04)

  print('Mean Absolute Error (Train): ', mae_rfr_reg_train)
  print('Mean Absolute Error (Test): ', mae_rfr_reg_test)
  print('Mean Squared Error (Train): ', mse_rfr_reg_train)
  print('Mean Squared Error (Test): ', mse_rfr_reg_test)
  print('Root Mean Squared Error (Train): ', rmse_rfr_reg_train)
  print('Root Mean Squared Error (Test): ', rmse_rfr_reg_test)
  print('R2 (Train): ', r2_rfr_reg_train)
  print('R2 (Test): ', r2_rfr_reg_test)
  print('Coefficient of Variance (Train): ', CoV_train_rfr)
  print('Coefficient of Variance (Test): ', CoV_test_rfr)

Random_forest_cv(X_train_5, y_train_5, X_test_5, y_test_5, X_5_2020)

def XGB_cv(X_train, y_train, X_test, y_test, X_pred):
  
  xgboost = XGBRegressor(objective='reg:squarederror')
  xgboost_cv = GridSearchCV(estimator=xgboost,
                            param_grid={"learning_rate": (0.05, 0.10, 0.15),
                                        "max_depth": [3, 4, 5, 6, 8],
                                        "min_child_weight": [ 1, 3, 5, 7],
                                        "gamma":[ 0.0, 0.1, 0.2],
                                        "colsample_bytree":[ 0.3, 0.4],},
                            cv=3, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)

  xgboost_cv.fit(X_train, y_train)
  best = xgboost_cv.best_params_

  xgboost_tuned = XGBRegressor(objective='reg:squarederror',
                               parameters = best)
  xgb_tuned =  xgboost_tuned.fit(X_train, y_train)

  train_preds_xgb_reg  = xgb_tuned.predict(X_train)
  test_preds_xgb_reg = xgb_tuned.predict(X_test)

  mae_xgb_reg_train = mean_absolute_error(y_train, train_preds_xgb_reg)
  mae_xgb_reg_test = mean_absolute_error(y_test, test_preds_xgb_reg)

  mse_xgb_reg_train = mean_squared_error(y_train, train_preds_xgb_reg)
  mse_xgb_reg_test = mean_squared_error(y_test, test_preds_xgb_reg) 

  rmse_xgb_reg_train = mean_squared_error(y_train, train_preds_xgb_reg , squared=False)
  rmse_xgb_reg_test = mean_squared_error(y_test, test_preds_xgb_reg, squared=False)

  r2_xgb_reg_train = r2_score(y_train, train_preds_xgb_reg)
  r2_xgb_reg_test = r2_score(y_test, test_preds_xgb_reg)

  explained_variance_xgb_train = explained_variance_score(y_train, train_preds_xgb_reg)
  explained_variance_xgb_test = explained_variance_score(y_test, test_preds_xgb_reg)

  CoV_train_xgb = rmse_xgb_reg_train/np.mean(y_train)
  CoV_test_xgb = rmse_xgb_reg_test/np.mean(y_test)

  predictions2020_xgb_reg = xgb_tuned.predict(X_pred)
  df_xgb = pd.DataFrame(predictions2020_xgb_reg, columns=['2020_Predictions'])  
  df_xgb_2020_01 = df_xgb.iloc[0:81,:].values
  df_xgb_2020_02 = df_xgb.iloc[81:163,:].values
  df_xgb_2020_03 = df_xgb.iloc[162:244,:].values
  df_xgb_2020_04 = df_xgb.iloc[244:325,:].values
  df_xgb_2020_01 = (pd.DataFrame(df_xgb_2020_01)).sum() 
  df_xgb_2020_02 = (pd.DataFrame(df_xgb_2020_02)).sum() 
  df_xgb_2020_03 = (pd.DataFrame(df_xgb_2020_03)).sum() 
  df_xgb_2020_04 = (pd.DataFrame(df_xgb_2020_04)).sum()

  print("Visualization of Predictions for Train:")
  predVisualization(train_preds_xgb_reg, y_train)
  print("Visualization of Predictions for Test:")
  predVisualization(test_preds_xgb_reg, y_test)

  print("Visualization of Error for Train:")
  errorVisualization(train_preds_xgb_reg, y_train)
  print("Visualization of Error for Test:")
  errorVisualization(test_preds_xgb_reg, y_test)

  print("Jan 2020 predictions: ", df_xgb_2020_01)
  print("Feb 2020 predictions: ", df_xgb_2020_02)
  print("March 2020 predictions: ", df_xgb_2020_03)
  print("Apr 2020 predictions: ", df_xgb_2020_04)

  print('Mean Absolute Error (Train): ', mae_xgb_reg_train)
  print('Mean Absolute Error (Test): ', mae_xgb_reg_test)
  print('Mean Squared Error (Train): ', mse_xgb_reg_train)
  print('Mean Squared Error (Test): ', mse_xgb_reg_test)
  print('Root Mean Squared Error (Train): ', rmse_xgb_reg_train)
  print('Root Mean Squared Error (Test): ', rmse_xgb_reg_test)
  print('R2 (Train): ', r2_xgb_reg_train)
  print('R2 (Test): ', r2_xgb_reg_test)
  print('Coefficient of Variance (Train): ', CoV_train_xgb)
  print('Coefficient of Variance (Test): ', CoV_test_xgb)

XGB_cv(X_train_5, y_train_5, X_test_5, y_test_5, X_5_2020)

DecisionTreeRegressor?

def decision_tree_cv(X_train, y_train, X_test, y_test, X_pred):

  dtr_reg = DecisionTreeRegressor()
  model_dtr = GridSearchCV(estimator=dtr_reg,
                           param_grid={"criterion": ["squared_error", "friedman_mse", "absolute_error", "poisson"],
                                        "min_samples_split": [ 2, 4, 6],
                                        "max_features":["auto", "sqrt", "log2"],},
                           scoring='neg_mean_squared_error',
                           cv=3, verbose=0, n_jobs=-1)

  model_dtr.fit(X_train, y_train)
  best = model_dtr.best_params_

  dtr_tuned = DecisionTreeRegressor(best)

  train_preds_dtr_reg  = dtr_tuned.predict(X_train)
  test_preds_dtr_reg = dtr_tuned.predict(X_test)

  mae_dtr_reg_train = mean_absolute_error(y_train, train_preds_dtr_reg)
  mae_dtr_reg_test = mean_absolute_error(y_test, test_preds_dtr_reg)

  mse_dtr_reg_train = mean_squared_error(y_train, train_preds_dtr_reg)
  mse_dtr_reg_test = mean_squared_error(y_test, test_preds_dtr_reg) 

  rmse_dtr_reg_train = mean_squared_error(y_train, train_preds_dtr_reg , squared=False)
  rmse_dtr_reg_test = mean_squared_error(y_test, test_preds_dtr_reg, squared=False)

  r2_dtr_reg_train = r2_score(y_train, train_preds_dtr_reg)
  r2_dtr_reg_test = r2_score(y_test, test_preds_dtr_reg)

  explained_variance_dtr_train = explained_variance_score(y_train, train_preds_dtr_reg)
  explained_variance_dtr_test = explained_variance_score(y_test, test_preds_dtr_reg)

  CoV_train_dtr = rmse_dtr_reg_train/np.mean(y_train)
  CoV_test_dtr = rmse_dtr_reg_test/np.mean(y_test)

  predictions2020_dtr_reg = dtr_reg.predict(X_pred)
  df_dtr = pd.DataFrame(predictions2020_dtr_reg, columns=['2020_Predictions'])  
  df_dtr_2020_01 = df_dtr.iloc[0:81,:].values
  df_dtr_2020_02 = df_dtr.iloc[81:163,:].values
  df_dtr_2020_03 = df_dtr.iloc[162:244,:].values
  df_dtr_2020_04 = df_dtr.iloc[244:325,:].values
  df_dtr_2020_01 = (pd.DataFrame(df_dtr_2020_01)).sum() 
  df_dtr_2020_02 = (pd.DataFrame(df_dtr_2020_02)).sum() 
  df_dtr_2020_03 = (pd.DataFrame(df_dtr_2020_03)).sum() 
  df_dtr_2020_04 = (pd.DataFrame(df_dtr_2020_04)).sum()

decision_tree_cv(X_train_5, y_train_5, X_test_5, y_test_5, X_5_2020)

"""It is clearly can be seean that most of error caused by outliers. In next chapter the effect of outliers will be examined.

***ELIMINATING OUTLIERS***

Since it is thought that the outliers of the dataset may have a negative effect on the results, the case of removing these values from the dataset was evaluated.
"""

# Eliminating outliers:
Q1 = ProductX['Quantity'].quantile(0.25)
Q3 = ProductX['Quantity'].quantile(0.75)
IQR = Q3-Q1
lower_bound = 0
upper_bound = Q3 + 1.5*IQR
inconsistent_values = (ProductX['Quantity'] < lower_bound) | (ProductX['Quantity'] > upper_bound)
index = ProductX[inconsistent_values].index
ProductX_wo_outliers = ProductX.drop(index)
ProductX_wo_outliers

# In this part the dataset restricted between 0 and upper bound of Quantity value.

ProductX_wo_outliers = ProductX_wo_outliers.reset_index(drop=True)
ProductX_wo_outliers

##################--- DataFrames for Train ---##################


# Dataset which excluded the Period, DolarB, Male_Rural variables:
X_1 = ProductX_wo_outliers.iloc[:2663, [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 ,13, 15, 16, 18]].values
y_1 = ProductX_wo_outliers.iloc[:2663, 17].values

# Dataset which excluded the Period, Province, DolarB, Male_Rural variables:
X_2 = ProductX_wo_outliers.iloc[:2663, [2, 3, 4, 5, 6, 7, 9, 10, 11 ,13, 15, 16, 18]].values
y_2 = ProductX_wo_outliers.iloc[:2663, 17].values

# Dataset which excluded the Period, Province, DolarB, Borsa, Male, Male_Rural variables:
X_3 = ProductX_wo_outliers.iloc[:2663, [2, 3, 4, 5, 6, 9, 10, 11 ,13, 15, 16, 18]].values
y_3 = ProductX_wo_outliers.iloc[:2663, 17].values

##################--- DataFrames for Test ---##################

# Dataset which excluded the Period, DolarB, Male_Rural variables:
X_1_2020 = ProductX_wo_outliers.iloc[2663:, [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 ,13, 15, 16, 18]].values
y_1_2020 = ProductX_wo_outliers.iloc[2663:, 17].values

# Dataset which excluded the Period, Province, DolarB, Male_Rural variables:
X_2_2020 = ProductX_wo_outliers.iloc[:2663, [2, 3, 4, 5, 6, 7, 9, 10, 11 ,13, 15, 16, 18]].values
y_2_2020 = ProductX_wo_outliers.iloc[2663:, 17].values

# Dataset which excluded the Period, Province, DolarB, Borsa, Male, Male_Rural variables:
X_3_2020 = ProductX_wo_outliers.iloc[2663:, [2, 3, 4, 5, 6, 9, 10, 11 ,13, 15, 16, 18]].values
y_3_2020 = ProductX_wo_outliers.iloc[2663:, 17].values

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_1 = scaler.fit_transform(X_1)
X_2 = scaler.fit_transform(X_2)
X_3 = scaler.fit_transform(X_3)

X_1_2020 = scaler.fit_transform(X_1_2020)
X_2_2020 = scaler.fit_transform(X_2_2020)
X_3_2020 = scaler.fit_transform(X_3_2020)

# Splitting dataset:
from sklearn.model_selection import train_test_split

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.3, shuffle=False, random_state = 0)
X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.3, shuffle=False, random_state = 0)
X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3, test_size=0.3, shuffle=False, random_state = 0)

XGB(X_train_1, y_train_1, X_test_1, y_test_1, X_1_2020)
XGB(X_train_2, y_train_2, X_test_2, y_test_2, X_2_2020)
XGB(X_train_3, y_train_3, X_test_3, y_test_3, X_3_2020)

"""The analyzes showed that the removal of outliers from the dataset both positively and negatively affected the results. Although the error metrics have decreased as desired, the significant decrease in the 2020 forecasts creates the feeling that the forecasts are wrong. For this reason, the predictions for 2020 were calculated without removing the outliers.

***MOVING AVERAGES***

Since the results obtained in the previous analyzes were not satisfactory, it was necessary to consider of adding new parameters to the dataset. In this section, three additional columns have been added to the main data set in order to better model the upward trend in sales data: Sales data of the previous month, sales data of three months ago, average sales quantity of the last three months.

Since the XGBOOST algorithm performs well in general and can work with missing data, it has been chosen in this section.
"""

ProductX_2 = ProductX.copy()

ProductX_2 = pd.read_csv('gdrive/My Drive/VitrA/Product_X_ALL_18092021.csv', sep=";")
df_heat = pd.read_excel('gdrive/My Drive/VitrA/20210914C0A8-Aylık Ortalama Sıcaklık (°C).xlsx')

ProductX_2['Quantity'] = ProductX_2['Quantity'].fillna(0)

ProductX_2 = ProductX_2[df.Quantity != 0]
ProductX_2 = ProductX_2.reset_index(drop=True)

df_last_2 = pd.merge(ProductX_2, df_heat)
df_last_2.reset_index(inplace=False)
df_last_2

ProductX_copy = df_last_2.copy()

ProductX_copy['CPI(Year)']=ProductX_copy['CPI(Year)'].str.replace(',','.')
ProductX_copy['CPI(Month)']=ProductX_copy['CPI(Month)'].str.replace(',','.')
ProductX_copy['Borsa']=ProductX_copy['Borsa'].str.replace(',','.')
ProductX_copy['DolarB']=ProductX_copy['DolarB'].str.replace(',','.')
ProductX_copy['DolarS']=ProductX_copy['DolarS'].str.replace(',','.')

ProductX_copy['CPI(Year)']=ProductX_copy['CPI(Year)'].astype(float)
ProductX_copy['CPI(Month)']=ProductX_copy['CPI(Month)'].astype(float)
ProductX_copy['Period'] = ProductX_copy['Period'].astype(int)
ProductX_copy['Year'] = ProductX_copy['Year'].astype(int)
ProductX_copy['Month'] = ProductX_copy['Month'].astype(int)
ProductX_copy['Borsa'] = ProductX_copy['Borsa'].astype(float)
ProductX_copy['DolarB'] = ProductX_copy['DolarB'].astype(float)
ProductX_copy['DolarS'] = ProductX_copy['DolarS'].astype(float)
ProductX_copy['Male'] = ProductX_copy['Male'].astype(float)
ProductX_copy['Male_Province'] = ProductX_copy['Male_Province'].astype(float)
ProductX_copy['Male_Rural'] = ProductX_copy['Male_Rural'].astype(float)

ProductX_copy['Province'] = labelencoder.fit_transform(ProductX_copy['Province'].values)
ProductX_copy['Season'] = labelencoder.fit_transform(ProductX_copy['Season'].values)
ProductX_copy['Region'] = labelencoder.fit_transform(ProductX_copy['Region'].values)

ProductX_2.head(245)

ProductX_copy.tail(325)

##################--- DataFrames for Train ---##################


# Dataset which excluded the Period, DolarB, Male_Rural variables:
X_1 = ProductX_copy.iloc[240:2881, [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 ,13, 15, 16, 18, 19, 20, 21]].values
y_1 = ProductX_copy.iloc[240:2881, 17].values

# Dataset which excluded the Period, Province, DolarB, Male_Rural variables:
X_2 = ProductX_copy.iloc[240:2881, [2, 3, 4, 5, 6, 7, 9, 10, 11 ,13, 15, 16, 18, 19, 20, 21]].values
y_2 = ProductX_copy.iloc[240:2881, 17].values

# Dataset which excluded the Period, Province, DolarB, Borsa, Male, Male_Rural variables:
X_3 = ProductX_copy.iloc[240:2881, [2, 3, 4, 5, 6, 9, 10, 11 ,13, 15, 16, 18, 19, 20, 21]].values
y_3 = ProductX_copy.iloc[240:2881, 17].values

##################--- DataFrames for Test ---##################

# Dataset which excluded the Period, DolarB, Male_Rural variables:
X_1_2020 = ProductX_copy.iloc[2881:, [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12 ,13, 15, 16, 18, 19, 20, 21]].values
y_1_2020 = ProductX_copy.iloc[2881:, 17].values

# Dataset which excluded the Period, Province, DolarB, Male_Rural variables:
X_2_2020 = ProductX_copy.iloc[:2881, [2, 3, 4, 5, 6, 7, 9, 10, 11 ,13, 15, 16, 18, 19, 20, 21]].values
y_2_2020 = ProductX_copy.iloc[2881:, 17].values

# Dataset which excluded the Period, Province, DolarB, Borsa, Male, Male_Rural variables:
X_3_2020 = ProductX_copy.iloc[2881:, [2, 3, 4, 5, 6, 9, 10, 11 ,13, 15, 16, 18, 19, 20, 21]].values
y_3_2020 = ProductX_copy.iloc[2881:, 17].values

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_1 = scaler.fit_transform(X_1)
X_2 = scaler.fit_transform(X_2)
X_3 = scaler.fit_transform(X_3)

X_1_2020 = scaler.fit_transform(X_1_2020)
X_2_2020 = scaler.fit_transform(X_2_2020)
X_3_2020 = scaler.fit_transform(X_3_2020)

# Splitting dataset:
from sklearn.model_selection import train_test_split

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.3, shuffle=False, random_state = 0)
X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.3, shuffle=False, random_state = 0)
X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y_3, test_size=0.3, shuffle=False, random_state = 0)

XGB(X_train_1, y_train_1, X_test_1, y_test_1, X_1_2020)
XGB(X_train_2, y_train_2, X_test_2, y_test_2, X_2_2020)
XGB(X_train_3, y_train_3, X_test_3, y_test_3, X_3_2020)

"""Since the estimates obtained from these analyzes are quite low compared to previous years, it can been seen that the added columns have a negative effect on the model. Therefore, all estimations were made using the main data set made in the first section.

***TIME SERIES***

Since sales data changes depending on time, it is thought that the time series method can be used besides machine learning models.
"""

from dateutil.parser import parse 
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

!pip install pmdarima

#ARIMA
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt
import pmdarima as pm
from pmdarima.model_selection import train_test_split

from fbprophet import Prophet

from pandas import to_datetime

import itertools
import warnings

import datetime
from datetime import datetime
warnings.filterwarnings('ignore')

df_ts = pd.read_csv('gdrive/My Drive/VitrA/TimeSeriesX.csv', sep=";")
df_ts

df_ts['Date'] = pd.to_datetime(df_ts['Date'])
df_ts

df_ts.dtypes

# Settin "Date" as index of DataFrame:

df_ts.set_index('Date',inplace=True)
df_ts

# Check point:

df_ts.to_csv('gdrive/My Drive/VitrA/ProductX_TS.csv')

"""Exploring data and it's trends:"""

# Time series data source: fpp pacakge in R.
import matplotlib.pyplot as plt

# Draw Plot
def plot_df(df, x, y, title="", xlabel='Date', ylabel='Quantity', dpi=100):
    plt.figure(figsize=(10,3), dpi=dpi)
    plt.plot(x, y, color='tab:red')
    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)
    plt.show()

plot_df(df_ts, x=df_ts.index, y=df_ts.Quantity, title='Annual Product X Sales from 2017 to 2019')

# Import data

df_a = pd.read_csv('gdrive/My Drive/VitrA/ProductX_TS.csv', parse_dates=['Date'])

x = df_a['Date'].values
y1 = df_a['Quantity'].values

# Plot
fig, ax = plt.subplots(1, 1, figsize=(10,3), dpi= 120)
plt.fill_between(x, y1=y1, y2=-y1, alpha=0.5, linewidth=2, color='seagreen')
plt.ylim(-25000, 25000)
plt.title('Quantity (Two Side View)', fontsize=16)
plt.hlines(y=0, xmin=np.min(df_a.Date), xmax=np.max(df_a.Date), linewidth=.5)
plt.show()

# Prepare data
df_a['year'] = [d.year for d in df_a.Date]
df_a['month'] = [d.strftime('%b') for d in df_a.Date]
years = df_a['year'].unique()

# Prep Colors
np.random.seed(100)
mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(years), replace=False)

# Draw Plot

for i, y in enumerate(years):
    if i >= 0:        
        plt.plot('month', 'Quantity', data=df_a.loc[df_a['year']==y, :], color=mycolors[i], label=y)
        plt.text(df_a.loc[df_a.year==y, :].shape[0]-.9, df_a.loc[df_a.year==y, 'Quantity'][-1:].values[0], y, fontsize=12, color=mycolors[i])

# Decoration
#plt.gca().set(xlim=(-0.3, 11), ylim=(2, 30), ylabel='$Drug Sales$', xlabel='$Month$')
plt.yticks(fontsize=12, alpha=.7)
plt.title("Seasonal Plot of Sales Time Series", fontsize=20)
plt.figure(figsize=(20,10))
plt.show()

# Draw Plot
fig, axes = plt.subplots(1, 2, figsize=(20,7), dpi= 80)
sns.boxplot(x='year', y='Quantity', data=df_a, ax=axes[0])
sns.boxplot(x='month', y='Quantity', data=df_a.loc[df_a.year.isin([2017, 2019]), :]) #?

# Set Title
axes[0].set_title('Year-wise Box Plot\n(The Trend)', fontsize=18); 
axes[1].set_title('Month-wise Box Plot\n(The Seasonality)', fontsize=18)
plt.show()

"""Checking the data if it is stationary or not:"""

from statsmodels.tsa.stattools import adfuller, kpss

# ADF Test
result = adfuller(df_a['Quantity'].values, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

# KPSS Test
result = kpss(df_a['Quantity'].values, regression='c')
print('\nKPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

"""Since the p values is greater than 0.05 the series is not stationary."""

from statsmodels.tsa.seasonal import seasonal_decompose
from dateutil.parser import parse

df_a = pd.read_csv('gdrive/My Drive/VitrA/ProductX_TS.csv', parse_dates=['Date'], index_col='Date')

# Multiplicative Decomposition 
result_mul = seasonal_decompose(df_a['Quantity'], model='multiplicative', extrapolate_trend='freq')

# Additive Decomposition
result_add = seasonal_decompose(df_a['Quantity'], model='additive', extrapolate_trend='freq')

# Plot
plt.rcParams.update({'figure.figsize': (10,10)})
result_mul.plot().suptitle('Multiplicative Decompose', fontsize=22)
result_add.plot().suptitle('Additive Decompose', fontsize=22)
plt.show()

# Extract the Components ----
# Actual Values = Product of (Seasonal * Trend * Resid)
df_reconstructed = pd.concat([result_mul.seasonal, result_mul.trend, result_mul.resid, result_mul.observed], axis=1)
df_reconstructed.columns = ['seas', 'trend', 'resid', 'actual_values']
df_reconstructed.head()

df_reconstructed = pd.concat([result_add.seasonal, result_add.trend, result_add.resid, result_add.observed], axis=1)
df_reconstructed.columns = ['seas', 'trend', 'resid', 'actual_values']
df_reconstructed.head()

from statsmodels.tsa.stattools import adfuller, kpss

# ADF Test
result = adfuller(df_a['Quantity'].values, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

# KPSS Test
result = kpss(df_a['Quantity'].values, regression='c')
print('\nKPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

X_train = df_a[0:25]
X_valid = df_a[24:36]

print('X_train Shape', X_train.shape)
print('X_Valid Shape', X_valid.shape)

# Using scipy: Subtract the line of best fit
from scipy import signal
detrended = signal.detrend(X_train.Quantity.values)
plt.plot(detrended)
plt.title('Quantities detrended by subtracting the least squares fit', fontsize=16)

"""Trying to transform the series to stationary with log:"""

from numpy import log
from matplotlib import pyplot
logTrans = log(df_a)
pyplot.plot(logTrans)

from statsmodels.tsa.stattools import adfuller, kpss

# ADF Test
result = adfuller(logTrans['Quantity'].values, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

# KPSS Test
result = kpss(logTrans['Quantity'].values, regression='c')
print('\nKPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

"""The p-value is still greater than 0.05. So differencing must be applied:"""

X_train_st = df_a[0:25]
X_valid_st = df_a[24:36]

print('X_train Shape', X_train.shape)
print('X_Valid Shape', X_valid.shape)

X_train_st = X_train_st.diff()
X_train_st.dropna(inplace=True)
X_train_st = X_train_st.diff()
X_train_st.dropna(inplace=True)

df_stationary = df_a.diff().dropna()
df_stationary

from statsmodels.tsa.stattools import adfuller, kpss

# ADF Test
result = adfuller(df_stationary['Quantity'].values, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

# KPSS Test
result = kpss(df_stationary['Quantity'].values, regression='c')
print('\nKPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

"""The p-value is lower than 0.05."""

X_train_st = df_stationary[0:25]
X_valid_st = df_stationary[24:36]

# Using scipy: Subtract the line of best fit
from scipy import signal
detrended = signal.detrend(X_train_st.Quantity.values)
plt.plot(detrended)
plt.title('Quantities detrended by subtracting the least squares fit', fontsize=16)

from statsmodels.tsa.stattools import acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf


# Calculate ACF and PACF upto 50 lags
# acf_50 = acf(df.value, nlags=50)
# pacf_50 = pacf(df.value, nlags=50)

# Draw Plot
fig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)
plot_acf(df_stationary.Quantity.tolist(), lags=24, ax=axes[0])
plot_pacf(df_stationary.Quantity.tolist(),lags=11, ax=axes[1])

# Setting the Index for 6 years
index_7_years = pd.date_range(X_train_st.index[-1], freq='MS', periods = 12) 

index_7_years

import itertools
p=d=q=range(0,11)
pdq = list(itertools.product(p,d,q))

for param in pdq:
    try:
        model_arima = ARIMA(X_train_st,order=param)
        model_arima_fit = model_arima.fit()
        print(param,model_arima_fit.aic)
    except:
        continue

from sklearn.metrics import mean_squared_error

# Running ARIMA with random numbers
model_arima = ARIMA(X_train_st, order=(0,2,2)) 
model_arima_fit = model_arima.fit(disp=-1)

# Saving ARIMA predictions
fcast1 = model_arima_fit.forecast(12)[0]

# Passing the same index as the others
fcast1 = pd.Series(fcast1, index=index_7_years)
fcast1 = fcast1.rename("Arima") 
print(fcast1)

# Ploting the predictions
#fig, ax = plt.subplots(figsize=(35,12))
#chart = sns.lineplot(x='Date', y='Quantity', data = X_train)
#chart.set_title('AU')
#fcast1.plot(ax=ax, color='red', marker="o", legend=True)
#X_valid.plot(ax=ax, color='blue', marker="o", legend=True)
print(fcast1.values)
print('The MSE of ARIMA is:', mean_squared_error(X_valid['Quantity'].values, fcast1.values, squared=False))

X_valid_st

X_valid_st['Quantity'].values

# Setting the Index for 6 years
index_4_years = pd.date_range(df_a.index[-1], freq='MS', periods = 5) 

index_4_years

# Running ARIMA with random numbers
model_arima = ARIMA(X_train_st, order=(0,2,2)) # start 2-0-0 and move to the best
model_arima_fit = model_arima.fit(disp=-1)

# Saving ARIMA predictions
fcast1 = model_arima_fit.forecast(5)[0]

# Passing the same index as the others
fcast1 = pd.Series(fcast1, index=index_4_years)
fcast1 = fcast1.rename("Arima") 
print(fcast1)

# Running auto ARIMA 
auto_arima_model = pm.auto_arima(df_a, seasonal=False, m=5)

# Read more about setting m
# https://alkaline-ml.com/pmdarima/tips_and_tricks.html

# make your forecasts
fcast2 = auto_arima_model.predict(5) 
fcast2 = pd.Series(fcast2, index=index_4_years)
fcast2 = fcast2.rename("Auto Arima")
print(fcast2)

"""The 2020 predictions with Auto ARIMA are similar to the ones with machine learning algorithms. So these are selected as the final results for time series model."""

df_base = pd.concat([df_a,df_a.shift(1)],axis=1)

df_base.dropna(inplace=True)

df_base.columns = ['Quantity','Shifted']
df_base

"""Summations of Quantity and Shifted values are much more greater than the ones previos models and the actual 2017-2019 sales. So this part of the model is neglected."""

df_base['Value'] = df_base['Quantity']+df_base['Shifted']
df_base

df_base.drop(['Quantity','Shifted'],axis=1,inplace=True)
df_base

plot_acf(df_base)

"""# **KERAS**

Since the desired results could not be obtained from machine learning algorithms and time series methods, the analyzes were also tested with KERAS. However there was no improvement in the results compared to other models.
"""

# Building model:

from keras import models
from keras import layers
def build_model():

  model = models.Sequential()
  model.add(layers.Dense(50, activation='relu',
                         input_shape=(X_train_1.shape[1],)))
  model.add(layers.Dense(50, activation='relu'))
  model.add(layers.Dense(50, activation='relu'))
  model.add(layers.Dense(1))
  model.compile(optimizer='Adam', loss='mse', metrics=['mae'])
  return model

# K-Fold cross validation:

import numpy as np
k = 3
num_val_samples = len(X_train_1) // k
num_epochs = 100
all_scores = []
for i in range(k):
  print('processing fold #', i)
  # Prepare the validation data: data from partition # k
  val_data = X_train_1[i * num_val_samples: (i + 1) * num_val_samples]
  val_targets = y_train_1[i * num_val_samples: (i + 1) * num_val_samples]
  # Prepare the training data: data from all other partitions
  partial_train_data = np.concatenate(
      [X_train_1[:i * num_val_samples],
       X_train_1[(i + 1) * num_val_samples:]],
       axis=0)
  partial_train_targets = np.concatenate(
      [y_train_1[:i * num_val_samples],
       y_train_1[(i + 1) * num_val_samples:]],
       axis=0)
  # Build the Keras model (already compiled)
  model = build_model()
  # Train the model (in silent mode, verbose=0)
  model.fit(partial_train_data, partial_train_targets,
            epochs=num_epochs, batch_size=1, verbose=0)
  # Evaluate the model on the validation data
  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
  all_scores.append(val_mae)

print(all_scores)
np.mean(all_scores)

num_epochs = 500
all_mae_histories = []
for i in range(k):
  print('processing fold #', i)
  # Prepare the validation data: data from partition # k
  val_data = X_train_1[i * num_val_samples: (i + 1) * num_val_samples]
  val_targets = y_train_1[i * num_val_samples: (i + 1) * num_val_samples]
  # Prepare the training data: data from all other partitions
  partial_train_data = np.concatenate(
      [X_train_1[:i * num_val_samples],
       X_train_1[(i + 1) * num_val_samples:]],
       axis=0)
  partial_train_targets = np.concatenate(
      [y_train_1[:i * num_val_samples],
       y_train_1[(i + 1) * num_val_samples:]],
       axis=0)
  # Build the Keras model (already compiled)
  model = build_model()
  # Train the model (in silent mode, verbose=0)
  history = model.fit(partial_train_data, partial_train_targets,
                      validation_data=(val_data, val_targets),
                      epochs=num_epochs, batch_size=1, verbose=0)
  mae_history = history.history['val_mae']
  all_mae_histories.append(mae_history)

# Averaging K-fold validation scores

average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]

# Plotting validation scores

import matplotlib.pyplot as plt
plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()

"""It can be seen that on 75-th100th epoch Mean Absolute Error of validation stops to improve. So the model will be build with that information:"""

# Get a fresh, compiled model.
model = build_model()
# Train it on the entirety of the data.
model.fit(X_train_1, y_train_1,
          epochs=100, batch_size=32, verbose=0)
test_mse_score, test_mae_score = model.evaluate(X_test_1, y_test_1)

"""Since there was no promising improvement in the results despite long operation time compared to other methods, neural networks with KERAS were not further examined and not reported in the main presentation."""